{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sc\n",
    "import os\n",
    "import csv\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split, GridSearchCV\n",
    "from copy import deepcopy\n",
    "from helper_py import load_karypis\n",
    "from sklearn.datasets import fetch_20newsgroups, load_iris\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from scipy.sparse import csr_matrix, vstack, hstack\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "#from tqdm import tnrange, tqdm_notebook\n",
    "from sklearn.datasets import load_files\n",
    "#import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_frame(rootdir):\n",
    "    i=0\n",
    "    data_frame = []\n",
    "    data_frame_target = []\n",
    "    target_hash, topic = {}, 0\n",
    "    for root, subFolders, files in os.walk(rootdir):\n",
    "        if(i==0):\n",
    "            subfold = subFolders       \n",
    "        if(i>0):\n",
    "            for filename in files: \n",
    "                fin = open(os.path.join(root,filename), 'r')\n",
    "                data=fin.read().replace('\\n', '')\n",
    "                data_frame.append(data)\n",
    "                if subfold[i-1] not in target_hash:\n",
    "                    target_hash[subfold[i-1]] = topic\n",
    "                    topic += 1\n",
    "                data_frame_target.append(target_hash[subfold[i-1]])\n",
    "                data = \"\"\n",
    "                fin.close()\n",
    "        i=i+1\n",
    "    return data_frame, np.array(data_frame_target,dtype=np.int32)\n",
    "rootdir_train = \"/run/media/avideep/Films/Important/MSc_Project/datasets/Reuters21578-Apte-90Cat/training/\"\n",
    "X_train,y_train = build_data_frame(rootdir_train)\n",
    "rootdir_test = \"/run/media/avideep/Films/Important/MSc_Project/datasets/Reuters21578-Apte-90Cat/test/\"\n",
    "X_test,y_test = build_data_frame(rootdir_test)\n",
    "tfidf = TfidfVectorizer(min_df=3)\n",
    "X_train = tfidf.fit_transform(X_train)\n",
    "X_test_new = []\n",
    "y_test_new = []\n",
    "i=0\n",
    "for idx, doc in enumerate(X_test):\n",
    "    try:\n",
    "        tfidf.transform(list(doc))\n",
    "        X_test_new.append(doc)\n",
    "        y_test_new.append(y_test[i])\n",
    "    except UnicodeDecodeError:\n",
    "        pass\n",
    "    i = i + 1\n",
    "y_test = y_test_new\n",
    "X_test = tfidf.transform(X_test_new)\n",
    "#X_train = features.fit_transform(X_train,y_train)\n",
    "#X_test = features.transform(X_test)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir_train = \"/run/media/avideep/Films/Important/MSc_Project/datasets/Reuters21578-Apte-90Cat/training/\"\n",
    "rootdir_test = \"/run/media/avideep/Films/Important/MSc_Project/datasets/Reuters21578-Apte-90Cat/test/\"\n",
    "train_data = load_files(rootdir_train)\n",
    "test_data = load_files(rootdir_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.data\n",
    "y_train = train_data.target\n",
    "X_test, y_test = test_data.data, test_data.target\n",
    "tfidf = TfidfVectorizer(min_df=3)\n",
    "X_train = tfidf.fit_transform(X_train)\n",
    "X_test_new = []\n",
    "y_test_new = []\n",
    "for idx, doc in enumerate(X_test):\n",
    "    try:\n",
    "        tfidf.transform(list(doc))\n",
    "        X_test_new.append(doc)\n",
    "        y_test_new.append(y_test[idx])\n",
    "    except UnicodeDecodeError:\n",
    "        pass\n",
    "y_test = []\n",
    "y_test = deepcopy(y_test_new)\n",
    "X_test = tfidf.transform(X_test_new)\n",
    "#X_train = features.fit_transform(X_train,y_train)\n",
    "#X_test = features.transform(X_test)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter(y_test)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 News Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroup_train = fetch_20newsgroups(subset='train')\n",
    "X_train = newsgroup_train.data\n",
    "y_train = newsgroup_train.target\n",
    "newsgroup_test = fetch_20newsgroups(subset='test')\n",
    "X_test = newsgroup_test.data\n",
    "y_test = newsgroup_test.target\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(min_df = 3)\n",
    "X_train = tfidf.fit_transform(X_train)\n",
    "X_test = tfidf.transform(X_test)\n",
    "features = SelectKBest(chi2, k = 1000)\n",
    "X_train = features.fit_transform(X_train,y_train)\n",
    "X_test = features.transform(X_test)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tknn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tknn(Xtrain,ytrain,xtest,beta,theta):\n",
    "    L = beta\n",
    "    S = []\n",
    "    class_names = np.unique(ytrain)\n",
    "    class_count = np.empty(class_names.shape[0])\n",
    "    for xtrain in Xtrain:\n",
    "        dist = distance.cosine(xtrain,xtest)\n",
    "        if dist > theta:\n",
    "            S.append(dist)\n",
    "    SPN_index = np.argsort(S)\n",
    "    SPN_labels = np.empty(SPN_index.shape[0]) \n",
    "    for i in range(SPN_index.shape[0]):\n",
    "        SPN_labels[i] = ytrain[SPN_index[i]]\n",
    "    SPN_0_labels = SPN_labels[:L]\n",
    "    for i in range(class_names.shape[0]):\n",
    "        val = 0\n",
    "        for j in range(SPN_0_labels.shape[0]):\n",
    "            if SPN_0_labels[j] == class_names[i]:\n",
    "                val = val + 1\n",
    "        class_count[i] = val\n",
    "    while L <= SPN_labels.shape[0]:\n",
    "        L_x1 = np.max(class_count)\n",
    "        L_x1_class = np.argmax(class_count)\n",
    "        L_x2 = np.max(np.delete(class_count,np.argmax(class_count)))\n",
    "        L_x2_class = np.argmax(np.delete(class_count,np.argmax(class_count)))\n",
    "        if L_x1 - L_x2 == beta:\n",
    "            return L_x1_class, L\n",
    "        else:\n",
    "            L = L + 1\n",
    "            c = SPN_labels[L]\n",
    "            #print(type(c))\n",
    "            class_count[int(c)] += 1\n",
    "    return -1, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X,y,number_of_class_labels = load_karypis('tr12',path=\"/run/media/avideep/Films/Important/MSc_Project/datasets/karypis\",min_df=3)\n",
    "sss = StratifiedShuffleSplit(n_splits=1,test_size=0.20,random_state=1)\n",
    "train_index, test_index = next(sss.split(X,y))\n",
    "X_train, X_test = X[train_index], X[test_index]\n",
    "y_train, y_test = y[train_index], y[test_index]\n",
    "tfidf = TfidfTransformer()\n",
    "X_train = tfidf.fit_transform(X_train)\n",
    "X_test = tfidf.transform(X_test)\n",
    "X_train, X_test = X_train.toarray(), X_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average value of L is 4\n",
      "\n",
      " Micro Averaged F1-Score :0.8095238095238095\n",
      "\n",
      " Accuracy Score:0.8095238095238095\n"
     ]
    }
   ],
   "source": [
    "predicted_class_label = list(np.empty(len(X_test)))\n",
    "L = list(np.empty(len(X_test)))\n",
    "for i in range(len(X_test)):\n",
    "    predicted_class_label[i], L[i] = tknn(X_train, y_train, X_test[i], \n",
    "                                    beta = 2,\n",
    "                                    theta = 0.001)\n",
    "    #print ('Classified test data #' + str(i) + ' as ' + str(predicted_class_label[i]) + ' with L = ' + str(L[i]))\n",
    "print('Average value of L is ' + str(int(np.mean(L) + 0.5)))\n",
    "fm=f1_score(y_test, predicted_class_label, average='micro') \n",
    "print ('\\n Micro Averaged F1-Score :'+str(fm))\n",
    "acc=accuracy_score(y_test, predicted_class_label) \n",
    "print ('\\n Accuracy Score:'+str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCI Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 1777)\n",
      "(200, 1777)\n"
     ]
    }
   ],
   "source": [
    "amazon_data_dir = '/run/media/avideep/Films/Important/MSc_Project/datasets/sentiment labelled sentences/amazon_cells_labelled.txt'\n",
    "imdb_data_dir = '/run/media/avideep/Films/Important/MSc_Project/datasets/sentiment labelled sentences/imdb_labelled.txt'\n",
    "yelp_data_dir = '/run/media/avideep/Films/Important/MSc_Project/datasets/sentiment labelled sentences/yelp_labelled.txt'\n",
    "sentiment_data = pd.read_csv(yelp_data_dir,sep='\\t',header=None)\n",
    "X,y = sentiment_data[0].values.tolist(), sentiment_data[1].values.tolist()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state =1)\n",
    "#sss = StratifiedShuffleSplit(n_splits=1,test_size=0.20,random_state=1)\n",
    "#train_index, test_index = next(sss.split(X,y))\n",
    "#X_train, X_test = X[train_index], X[test_index]\n",
    "#y_train, y_test = y[train_index], y[test_index]\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train = tfidf.fit_transform(X_train)\n",
    "X_test = tfidf.transform(X_test)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spambase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spambase_data_dir = '/run/media/avideep/Films/Important/MSc_Project/datasets/spambase/spambase.data'\n",
    "spam_data = pd.read_csv(spambase_data_dir,sep=',',header=None)\n",
    "#X,y = spam_data[0].values.tolist(), sentiment_data[1].values.tolist()\n",
    "y = spam_data[57].values.tolist()\n",
    "X = spam_data.drop([57],axis = 1).values.tolist()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Karypis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X,y,number_of_class_labels = load_karypis('tr45',path=\"/media/avideep/Films/Important/MSc_Project/datasets/karypis\",min_df=3)\n",
    "sss = StratifiedShuffleSplit(n_splits=1,test_size=0.20,random_state=1)\n",
    "train_index, test_index = next(sss.split(X,y))\n",
    "X_train, X_test = X[train_index], X[test_index]\n",
    "y_train, y_test = y[train_index], y[test_index]\n",
    "tfidf = TfidfTransformer()\n",
    "X_train = tfidf.fit_transform(X_train)\n",
    "X_test = tfidf.transform(X_test)\n",
    "X_train, X_test = X_train.toarray(), X_test.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X,y = iris.data, iris.target\n",
    "sss = StratifiedShuffleSplit(n_splits=1,test_size=0.20,random_state=1)\n",
    "train_index, test_index = next(sss.split(X,y))\n",
    "X_train, X_test = X[train_index], X[test_index]\n",
    "y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer UCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "X,y = data.data, data.target\n",
    "sss = StratifiedShuffleSplit(n_splits=1,test_size=0.20,random_state=1)\n",
    "train_index, test_index = next(sss.split(X,y))\n",
    "X_train, X_test = X[train_index], X[test_index]\n",
    "y_train, y_test = y[train_index], y[test_index]\n",
    "print(X.shape)\n",
    "print(len(data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ecoli UCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(336, 7)\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "ecoli_data_dir = '/run/media/avideep/Films/Important/MSc_Project/datasets/ecoli.data'\n",
    "data = pd.read_csv(ecoli_data_dir,sep=',',header=None)\n",
    "X = np.array(data.drop([0,data.shape[1]-1],axis=1).values.tolist())\n",
    "a = list(np.unique(data.iloc[:,-1]))\n",
    "le = LabelEncoder()\n",
    "le.fit(a)\n",
    "y = le.transform(data.iloc[:,-1].values.tolist())\n",
    "sss = StratifiedShuffleSplit(n_splits=1,test_size=0.20,random_state=1)\n",
    "train_index, test_index = next(sss.split(X,y))\n",
    "X_train, X_test = X[train_index], X[test_index]\n",
    "y_train, y_test = y[train_index], y[test_index]\n",
    "print(X.shape)\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ionosphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(351, 34)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "ionosphere_data_dir = '/run/media/avideep/Films/Important/MSc_Project/datasets/ionosphere.data'\n",
    "data = pd.read_csv(ionosphere_data_dir,sep=',',header=None)\n",
    "X = np.array(data.drop([data.shape[1]-1],axis=1).values.tolist())\n",
    "a = list(np.unique(data.iloc[:,-1]))\n",
    "le = LabelEncoder()\n",
    "le.fit(a)\n",
    "y = le.transform(data.iloc[:,-1].values.tolist())\n",
    "sss = StratifiedShuffleSplit(n_splits=1,test_size=0.20,random_state=1)\n",
    "train_index, test_index = next(sss.split(X,y))\n",
    "X_train, X_test = X[train_index], X[test_index]\n",
    "y_train, y_test = y[train_index], y[test_index]\n",
    "print(X.shape)\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# abalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4177, 7)\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "abalone_data_dir = '/run/media/avideep/Films/Important/MSc_Project/datasets/abalone.txt'\n",
    "data = pd.read_csv(abalone_data_dir,sep=',',header=None)\n",
    "X = np.array(data.drop([0,data.shape[1]-1],axis=1).values.tolist())\n",
    "a = list(np.unique(data.iloc[:,-1]))\n",
    "le = LabelEncoder()\n",
    "le.fit(a)\n",
    "y = le.transform(data.iloc[:,-1].values.tolist())\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state =1)\n",
    "print(X.shape)\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pima India Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 8)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "pid_data_dir = '/run/media/avideep/Films/Important/MSc_Project/datasets/PimaIndiaDiabetes.txt'\n",
    "data = pd.read_csv(pid_data_dir,sep=',',header=None)\n",
    "X = np.array(data.drop([data.shape[1]-1],axis=1).values.tolist())\n",
    "a = list(np.unique(data.iloc[:,-1]))\n",
    "le = LabelEncoder()\n",
    "le.fit(a)\n",
    "y = le.transform(data.iloc[:,-1].values.tolist())\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state =1)\n",
    "print(X.shape)\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vowel Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowel_data_dir = '/run/media/avideep/Films/Important/MSc_Project/datasets/vowel-context.data'\n",
    "data = pd.read_csv(vowel_data_dir,sep=',',header=None)\n",
    "data_train = data[data[0] == 0]\n",
    "data_test = data[data[0] == 1]\n",
    "X_train = np.array(data_train.drop([0,data.shape[1]-1],axis=1).values.tolist())\n",
    "X_test = np.array(data_test.drop([0,data.shape[1]-1],axis=1).values.tolist())\n",
    "y_train = data_train.iloc[:,-1]\n",
    "y_test = data_test.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# glass UCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214, 9)\n"
     ]
    }
   ],
   "source": [
    "glass_data_dir = '/run/media/avideep/Films/Important/MSc_Project/datasets/glass.data'\n",
    "data = pd.read_csv(glass_data_dir,sep=',',header=None)\n",
    "X = np.array(data.drop([0,data.shape[1]-1],axis=1).values.tolist())\n",
    "y =np.array(data.iloc[:,-1].values.tolist())\n",
    "sss = StratifiedShuffleSplit(n_splits=1,test_size=0.20,random_state=1)\n",
    "train_index, test_index = next(sss.split(X,y))\n",
    "X_train, X_test = X[train_index], X[test_index]\n",
    "y_train, y_test = y[train_index], y[test_index]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lung Cancer UCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 54)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "lung_data_dir = '/run/media/avideep/Films/Important/MSc_Project/datasets/lung-cancer.data'\n",
    "data = pd.read_csv(lung_data_dir,sep=',',header=None)\n",
    "data_dropped = data.drop([4,38],axis = 1)\n",
    "X = np.array(data_dropped.drop([0],axis=1).values.tolist())\n",
    "y = np.array(data_dropped[0].values.tolist())\n",
    "sss = StratifiedShuffleSplit(n_splits=1,test_size=0.20,random_state=1)\n",
    "train_index, test_index = next(sss.split(X,y))\n",
    "X_train, X_test = X[train_index], X[test_index]\n",
    "y_train, y_test = y[train_index], y[test_index]\n",
    "print(X.shape)\n",
    "print(len(np.unique(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "lung_data_dir = '/run/media/avideep/Films/Important/MSc_Project/datasets/processed.cleveland.data'\n",
    "data = pd.read_csv(lung_data_dir,sep=',',header=None)\n",
    "X = np.array(data.drop([data.shape[1]-1],axis=1).values.tolist())\n",
    "y =np.array(data.iloc[:,-1].values.tolist())\n",
    "sss = StratifiedShuffleSplit(n_splits=1,test_size=0.20,random_state=1)\n",
    "train_index, test_index = next(sss.split(X,y))\n",
    "X_train, X_test = X[train_index], X[test_index]\n",
    "y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Segmentation UCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class_names = list(np.unique(y_train))\n",
    "class_names_dict = {}\n",
    "for i,val in enumerate(class_names):\n",
    "    class_names_dict[i] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def findMedians(X_train,y_train):\n",
    "    medians = {}\n",
    "    for class_name in class_names:\n",
    "        class_name_x_train = [X_train[i] for i,val in enumerate(y_train) if val == class_name]\n",
    "        mean = []\n",
    "        for i in range(len(class_name_x_train[0])):\n",
    "            col = []\n",
    "            for row in class_name_x_train:\n",
    "                col.append(row[i])\n",
    "            mean.append(np.mean(np.array(col).astype(np.float)))\n",
    "        median_row = deepcopy(class_name_x_train[0])\n",
    "        for row in class_name_x_train:\n",
    "            if distance.cosine(np.array(row).astype(np.float),np.array(mean).astype(np.float)) < distance.cosine(np.array(median_row).astype(np.float),np.array(mean).astype(np.float)):\n",
    "                median_row = deepcopy(row)\n",
    "        medians[class_name] = np.array(median_row).astype(np.float)\n",
    "    return medians\n",
    "\n",
    "def findSelectedSamples(X_train,y_train,x_test,medians,method2):\n",
    "    X_train = np.array(X_train).astype(np.float)\n",
    "    x_test = np.array(x_test).astype(np.float)\n",
    "    x_train_selected = []\n",
    "    y_train_selected = []\n",
    "    if method2 == 1: \n",
    "        median_list = list(np.empty(len(class_names)))\n",
    "        for i in range(len(class_names)):\n",
    "            median_list[i] = distance.cosine(medians[class_names_dict[i]],x_test)\n",
    "        min_median = max(median_list)\n",
    "        for i in range(len(X_train)):\n",
    "            if(distance.cosine(X_train[i],x_test)<=distance.cosine(min_median,x_test)):\n",
    "            #if(distance.cosine(X_train[i],x_test)<=distance.cosine(medians[y_train[i]],x_test)):\n",
    "                x_train_selected.append(X_train[i])\n",
    "                y_train_selected.append(y_train[i])\n",
    "    elif method2 == 0:\n",
    "        for i in range(len(X_train)):\n",
    "            #if(distance.cosine(X_train[i],x_test)<=distance.cosine(min_median,x_test)):\n",
    "            if(distance.cosine(X_train[i],x_test)<=distance.cosine(medians[y_train[i]],x_test)):\n",
    "                x_train_selected.append(X_train[i])\n",
    "                y_train_selected.append(y_train[i])\n",
    "    return np.array(x_train_selected), y_train_selected\n",
    "\n",
    "def sortneighbors(x,y,x_test):\n",
    "    x = np.array(x).astype(np.float)\n",
    "    x_test = np.array(x_test).astype(np.float)\n",
    "    dist = np.empty(len(x))\n",
    "    for i in range(len(x)):\n",
    "        dist[i] = distance.cosine(x[i],x_test)\n",
    "    dist = np.argsort(dist)\n",
    "    x_sorted = np.empty(shape = (len(x),len(X_train[1])))\n",
    "    y_sorted = []\n",
    "    k = 0\n",
    "    for i in dist:\n",
    "        x_sorted[k] = x[i]\n",
    "        y_sorted.append(y[i])\n",
    "        k = k + 1\n",
    "    return x_sorted,y_sorted\n",
    "\n",
    "def cosine_weighted_tkNN(X_train,y_train,x_test,medians,gamma=0.025,l=2,method=1,method2=1):\n",
    "    y_test = -1\n",
    "    x_train_selected, y_train_selected = findSelectedSamples(X_train,y_train,x_test,medians,method2) \n",
    "    x_train_sorted, y_train_sorted = sortneighbors(x_train_selected,y_train_selected,x_test)\n",
    "    x_test = np.array(x_test).astype(np.float)  \n",
    "    x_train_sorted = np.array(x_train_sorted).astype(np.float)\n",
    "    L = l\n",
    "    scaler = MinMaxScaler()\n",
    "    while(L <= len(x_train_sorted)):\n",
    "        S_L = x_train_sorted[:L]\n",
    "        S_L_y = y_train_sorted[:L]\n",
    "        weights = np.empty(len(S_L))\n",
    "        weights_temp = []\n",
    "        if method == 1:\n",
    "            for i in range(len(S_L)):\n",
    "                weights_temp.append([distance.cosine(S_L[i],x_test), distance.cosine(medians[S_L_y[i]],S_L[i])])\n",
    "            weights_transformed = scaler.fit_transform(weights_temp)\n",
    "            for i in range(len(S_L)):\n",
    "                #weights[i] = ((1 - distance.cosine(S_L[i],x_test))*(1 - distance.cosine(medians[S_L_y[i]],x_test)))\n",
    "                weights[i] = (1 - weights_transformed[i][0]) * (1 - weights_transformed[i][1])\n",
    "        elif method == 0:\n",
    "            for i in range(len(S_L)):\n",
    "                weights[i] = ((1 - distance.cosine(S_L[i],x_test))*(1 - distance.cosine(medians[S_L_y[i]],x_test)))\n",
    "        class_weights = np.zeros(len(class_names))\n",
    "        class_cardinality = np.zeros(len(class_names))\n",
    "        p = 0\n",
    "        for class_name in class_names:\n",
    "            class_weights[p] = np.sum([weights[k] for k in np.where(S_L_y == class_name)])\n",
    "            class_cardinality[p] = np.sum([1 for k in np.where(S_L_y == class_name)])                                                   \n",
    "            p = p + 1 \n",
    "        max1 = max(class_weights)\n",
    "        max1_index = np.argmax(class_weights)\n",
    "        if class_cardinality[max1_index] == 0:\n",
    "            nmax1 = 0\n",
    "        else:\n",
    "            nmax1 = max1 / class_cardinality[max1_index]\n",
    "        class_weights_max_containing = list(class_weights)\n",
    "        class_weights_temp = list(class_weights)\n",
    "        class_weights_temp.remove(max1)\n",
    "        max2 = max(class_weights_temp)\n",
    "        max2_index = np.argmax(class_weights_temp)\n",
    "        if class_cardinality[max2_index] == 0:\n",
    "            nmax2 = 0\n",
    "        else:\n",
    "            nmax2 = max2 / class_cardinality[max2_index]\n",
    "        if(nmax1 - nmax2 >=gamma):\n",
    "            y_test = class_names[class_weights_max_containing.index(max1)]\n",
    "            break\n",
    "        else:\n",
    "            L = L + 1\n",
    "    return y_test, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train.toarray(), X_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'findMedians' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-37a6d3aca84c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmedians\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindMedians\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'findMedians' is not defined"
     ]
    }
   ],
   "source": [
    "medians = findMedians(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified test data #0 as (4, 3) with L = 6.91075581265556e-310\n",
      "Classified test data #1 as (7, 9) with L = 6.91075581265556e-310\n",
      "Classified test data #2 as (4, 3) with L = 4.6439151246985e-310\n",
      "Classified test data #3 as (2, 3) with L = 4.6439151246985e-310\n",
      "Classified test data #4 as (1, 3) with L = 0.0\n",
      "Classified test data #5 as (3, 3) with L = 0.0\n",
      "Classified test data #6 as (8, 3) with L = nan\n",
      "Classified test data #7 as (6, 3) with L = 0.0\n",
      "Classified test data #8 as (0, 3) with L = 0.0\n",
      "Classified test data #9 as (1, 3) with L = 5.4e-323\n",
      "Classified test data #10 as (4, 3) with L = 6.91068361762017e-310\n",
      "Classified test data #11 as (8, 3) with L = 4.6439133248577e-310\n",
      "Classified test data #12 as (4, 3) with L = 0.0\n",
      "Classified test data #13 as (3, 3) with L = 0.0\n",
      "Classified test data #14 as (0, 3) with L = nan\n",
      "Classified test data #15 as (3, 3) with L = 0.0\n",
      "Classified test data #16 as (5, 3) with L = 3.16e-322\n",
      "Classified test data #17 as (0, 3) with L = 5.4e-323\n",
      "Classified test data #18 as (9, 3) with L = 6.9106836748717e-310\n",
      "Classified test data #19 as (8, 5) with L = 4.64391332737287e-310\n",
      "Classified test data #20 as (8, 3) with L = 0.0\n",
      "Classified test data #21 as (6, 3) with L = 1.4e-322\n",
      "Classified test data #22 as (2, 3) with L = nan\n",
      "Classified test data #23 as (1, 3) with L = 0.0\n",
      "Classified test data #24 as (4, 14) with L = 6.3e-322\n",
      "Classified test data #25 as (6, 3) with L = 5.4e-323\n",
      "Classified test data #26 as (4, 7) with L = 6.9106836748717e-310\n",
      "Classified test data #27 as (4, 3) with L = 4.64391332737366e-310\n",
      "Classified test data #28 as (4, 3) with L = nan\n",
      "Classified test data #29 as (3, 3) with L = 1.6e-322\n",
      "Classified test data #30 as (4, 3) with L = nan\n",
      "Classified test data #31 as (4, 3) with L = 0.0\n",
      "Classified test data #32 as (6, 3) with L = 9.5e-322\n",
      "Classified test data #33 as (3, 3) with L = 5.4e-323\n",
      "Classified test data #34 as (0, 3) with L = 6.9106836748717e-310\n",
      "Classified test data #35 as (3, 3) with L = 4.64391332737406e-310\n",
      "Classified test data #36 as (3, 3) with L = 0.0\n",
      "Classified test data #37 as (8, 3) with L = 1.6e-322\n",
      "Classified test data #38 as (3, 3) with L = nan\n",
      "Classified test data #39 as (4, 3) with L = 5e-323\n",
      "Classified test data #40 as (3, 8) with L = 1.265e-321\n",
      "Classified test data #41 as (4, 3) with L = 4e-323\n",
      "Classified test data #42 as (8, 3) with L = 6.9106836748719e-310\n",
      "Classified test data #43 as (3, 11) with L = 4.6439133274292e-310\n",
      "Classified test data #44 as (1, 5) with L = 0.0\n",
      "Classified test data #45 as (6, 3) with L = 6e-323\n",
      "Classified test data #46 as (2, 3) with L = nan\n",
      "Classified test data #47 as (0, 3) with L = 0.0\n",
      "Classified test data #48 as (9, 3) with L = 1.58e-321\n",
      "Classified test data #49 as (4, 3) with L = 5.4e-323\n",
      "Classified test data #50 as (4, 7) with L = 6.9106836748717e-310\n",
      "Classified test data #51 as (3, 17) with L = 4.64391332725785e-310\n",
      "Classified test data #52 as (3, 3) with L = 0.0\n",
      "Classified test data #53 as (0, 3) with L = 9e-323\n",
      "Classified test data #54 as (2, 3) with L = nan\n",
      "Classified test data #55 as (8, 3) with L = 6.9104914928598e-310\n",
      "Classified test data #56 as (3, 6) with L = 1.897e-321\n",
      "Classified test data #57 as (8, 3) with L = 4.4e-323\n",
      "Classified test data #58 as (3, 16) with L = 6.9106836748711e-310\n",
      "Classified test data #59 as (3, 3) with L = 4.6439133272958e-310\n",
      "Classified test data #60 as (8, 3) with L = nan\n",
      "Classified test data #61 as (1, 3) with L = 6e-323\n",
      "Classified test data #62 as (4, 3) with L = nan\n",
      "Classified test data #63 as (3, 3) with L = 6.9106854607526e-310\n",
      "Classified test data #64 as (4, 5) with L = 2.213e-321\n",
      "Classified test data #65 as (1, 5) with L = 3.5e-323\n",
      "Classified test data #66 as (4, 6) with L = 6.9106836748711e-310\n",
      "Classified test data #67 as (3, 9) with L = 4.6439133273031e-310\n",
      "Classified test data #68 as (9, 3) with L = 0.0\n",
      "Classified test data #69 as (2, 3) with L = 9.4e-323\n",
      "Classified test data #70 as (3, 3) with L = 5e-323\n",
      "Classified test data #71 as (4, 3) with L = 6.9106854607526e-310\n",
      "Classified test data #72 as (4, 3) with L = 2.53e-321\n",
      "Classified test data #73 as (4, 11) with L = 1.5e-323\n",
      "Classified test data #74 as (3, 3) with L = 6.9106836748711e-310\n",
      "Classified test data #75 as (1, 9) with L = 4.6439133274292e-310\n",
      "Classified test data #76 as (2, 3) with L = 0.0\n",
      "Classified test data #77 as (4, 3) with L = 0.0\n",
      "Classified test data #78 as (4, 3) with L = 6.9106836748705e-310\n",
      "Classified test data #79 as (3, 3) with L = 0.0\n",
      "Classified test data #80 as (9, 3) with L = 0.0\n",
      "Classified test data #81 as (3, 3) with L = 0.0\n",
      "Classified test data #82 as (3, 3) with L = 0.0\n",
      "Classified test data #83 as (9, 3) with L = 0.0\n",
      "Classified test data #84 as (4, 3) with L = 0.0\n",
      "Classified test data #85 as (4, 3) with L = 0.0\n",
      "Classified test data #86 as (3, 17) with L = 0.0\n",
      "Classified test data #87 as (3, 3) with L = 0.0\n",
      "Classified test data #88 as (8, 3) with L = 6.9106836748705e-310\n",
      "Classified test data #89 as (8, 3) with L = 0.0\n",
      "Classified test data #90 as (8, 3) with L = 6.9106836748705e-310\n",
      "Classified test data #91 as (8, 3) with L = 6.9106836748711e-310\n",
      "Classified test data #92 as (3, 3) with L = 0.0\n",
      "Classified test data #93 as (6, 3) with L = 2.53e-321\n",
      "Classified test data #94 as (0, 3) with L = 1e-323\n",
      "Classified test data #95 as (3, 3) with L = 6.9106836748711e-310\n",
      "Classified test data #96 as (4, 9) with L = 4.643913327297e-310\n",
      "Classified test data #97 as (8, 3) with L = 6.9106836748709e-310\n",
      "Classified test data #98 as (0, 3) with L = 4.64391332726734e-310\n",
      "Classified test data #99 as (3, 9) with L = 0.0\n",
      "Classified test data #100 as (4, 3) with L = 2.213e-321\n",
      "Classified test data #101 as (4, 3) with L = 1e-323\n",
      "Classified test data #102 as (0, 3) with L = 6.9106836748656e-310\n",
      "Classified test data #103 as (1, 3) with L = 4.64391332739264e-310\n",
      "Classified test data #104 as (4, 11) with L = 6.79038653267e-313\n",
      "Classified test data #105 as (4, 3) with L = 6.79038653267e-313\n",
      "Classified test data #106 as (2, 5) with L = 6.79038653267e-313\n",
      "Classified test data #107 as (2, 3) with L = 2.079555875304e-312\n",
      "Classified test data #108 as (3, 3) with L = 2.46151511802e-312\n",
      "Classified test data #109 as (1, 14) with L = 6.7903865359e-313\n",
      "Classified test data #110 as (8, 3) with L = 6.7903865341e-313\n",
      "Classified test data #111 as (4, 3) with L = 9.33678148277e-313\n",
      "Classified test data #112 as (4, 3) with L = 6.7903865316e-313\n",
      "Classified test data #113 as (5, 3) with L = 6.79038653267e-313\n",
      "Classified test data #114 as (4, 3) with L = 6.79038653267e-313\n",
      "Classified test data #115 as (3, 3) with L = 6.79038653267e-313\n",
      "Classified test data #116 as (2, 3) with L = 6.79038653267e-313\n",
      "Classified test data #117 as (8, 3) with L = 6.79038653267e-313\n",
      "Classified test data #118 as (3, 3) with L = 6.79038653267e-313\n",
      "Classified test data #119 as (6, 3) with L = 6.79038653267e-313\n",
      "Classified test data #120 as (6, 3) with L = 6.79038653267e-313\n",
      "Classified test data #121 as (3, 3) with L = 6.79038653267e-313\n",
      "Classified test data #122 as (8, 3) with L = 6.79038653267e-313\n",
      "Classified test data #123 as (9, 3) with L = 6.79038653267e-313\n",
      "Classified test data #124 as (4, 3) with L = 6.79038653267e-313\n",
      "Classified test data #125 as (4, 7) with L = 6.79038653267e-313\n",
      "Classified test data #126 as (4, 3) with L = 6.79038653267e-313\n",
      "Classified test data #127 as (4, 3) with L = 6.79038653267e-313\n",
      "Classified test data #128 as (4, 3) with L = 6.79038653267e-313\n",
      "Classified test data #129 as (8, 3) with L = 6.79038653267e-313\n",
      "Classified test data #130 as (0, 3) with L = 2.46151511768e-312\n",
      "Classified test data #131 as (3, 3) with L = 2.14321574939e-312\n",
      "Classified test data #132 as (4, 3) with L = 2.05833591781e-312\n",
      "Classified test data #133 as (9, 3) with L = 1.294417432647e-312\n",
      "Classified test data #134 as (3, 3) with L = 1.01855797982e-312\n",
      "Classified test data #135 as (3, 3) with L = 1.01855797989e-312\n",
      "Classified test data #136 as (1, 3) with L = 1.03977793781e-312\n",
      "Classified test data #137 as (4, 3) with L = 2.121995793e-313\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-2309e2a83b14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                                     theta = 0.001)\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Classified test data #'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' as '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_class_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' with L = '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Average value of L is '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "predicted_class_label = list(np.empty(len(X_test)))\n",
    "L = list(np.empty(len(X_test)))\n",
    "for i in range(len(X_test)):\n",
    "    predicted_class_label[i] = tknn(X_train, y_train, X_test[i], \n",
    "                                    beta = 3,\n",
    "                                    theta = 0.001)\n",
    "    print ('Classified test data #' + str(i) + ' as ' + str(predicted_class_label[i]) + ' with L = ' + str(L[i]))\n",
    "print('Average value of L is ' + str(int(np.mean(L) + 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification of the test samples\n",
      "\n",
      " Macro Averaged F1-Score :0.8377046400797787\n",
      "\n",
      " Accuracy Score:0.9202898550724637\n"
     ]
    }
   ],
   "source": [
    "#predicted_class_label_temp = [predicted_class_label[i] for i,val in enumerate(predicted_class_label) if val != -1]\n",
    "#predicted_class_label = deepcopy(predicted_class_label_temp)\n",
    "#y_test_temp = [y_test[i] for i,val in enumerate(predicted_class_label) if val != -1]\n",
    "#y_test = deepcopy(y_test_temp)\n",
    "print ('Classification of the test samples')\n",
    "# print ('Evaluation using Precision, Recall and F-measure (micro)')  \n",
    "# pr=precision_score(y_test, predicted_class_label, average='micro')\n",
    "# print ('\\n Precision:'+str(pr))\n",
    "# re=recall_score(y_test, predicted_class_label, average='micro')\n",
    "# print ('\\n Recall:'+str(re))\n",
    "# fm=f1_score(y_test, predicted_class_label, average='micro') \n",
    "# print ('\\n F-measure:'+str(fm))\n",
    "\n",
    "# print ('Evaluation using Precision, Recall and F-measure (macro)')\n",
    "# pr=precision_score(y_test, predicted_class_label, average='macro')\n",
    "# print ('\\n Precision:'+str(pr))\n",
    "# re=recall_score(y_test, predicted_class_label, average='macro')\n",
    "# print ('\\n Recall:'+str(re))\n",
    "fm=f1_score(y_test, predicted_class_label, average='macro') \n",
    "print ('\\n Macro Averaged F1-Score :'+str(fm))\n",
    "acc=accuracy_score(y_test, predicted_class_label) \n",
    "print ('\\n Accuracy Score:'+str(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 123,
   "position": {
    "height": "558px",
    "left": "-13px",
    "right": "1150px",
    "top": "51px",
    "width": "229px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
